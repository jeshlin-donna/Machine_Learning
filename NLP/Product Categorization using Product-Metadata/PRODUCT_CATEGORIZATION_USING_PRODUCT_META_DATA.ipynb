{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PRODUCT CATEGORIZATION USING PRODUCT META DATA.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "lBxvxIrdYOq9",
        "MfCxqeZlsH0t",
        "6oH4aBCDmBWl",
        "oiFi3dArwzi7",
        "cicVAMJ9qU9s"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYGsTEjmGvef"
      },
      "source": [
        "**NOTE:**\n",
        "This notebook was used to try out various models for the product categorization challenge as part of the **DL Hackathon conducted by Analytics Club of IITM in association with Tech-Soc IITM**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L8IGmYueeiF"
      },
      "source": [
        "**NOTE:**\n",
        "\n",
        "Various kinds of models including Transformers(inferance time 5mins), Bert(inferance time 12 mins) and LSTM based models were tried out. The deep learning based models gave a slight rise in accuracy score with a huge inferance time whereas the LinearSVM model(with a TgdifVectorizer) was found to have a significant accuracy very close to the above mentioned models in a very minimal inferance time of 23 secs for predicting about 8k test datapoints."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swLA_XqZc_nl"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdOl2uVmagz0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgjWjP9hbrIn"
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\"\n",
        "# /content/gdrive/My Drive/Kaggle is the path where kaggle.json is present in the Google Drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwwtXAk5bBfC"
      },
      "source": [
        "! kaggle competitions download -c techsoc-analytics-21-22 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOMo39pXb18y"
      },
      "source": [
        "#unzipping the zip files and deleting the zip files\n",
        "!unzip \\test.csv.zip  && rm test.csv.zip\n",
        "!unzip \\train.csv.zip  && rm train.csv.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsdWcTcBcc8N"
      },
      "source": [
        "df=pd.read_csv('/content/train.csv')\n",
        "sample_submission=pd.read_csv('/content/sample_submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GnkuWWck3b7"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqmK7NcC3SaB"
      },
      "source": [
        "df['target_ind'].unique().shape #we have to categorize using the captions into these 500 categories"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBxvxIrdYOq9"
      },
      "source": [
        "#LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CW30GtjWZMgT"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Dropout\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "from bs4 import BeautifulSoup\n",
        "import plotly.graph_objs as go"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHZl-oSsYRqL"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "df['content'] = df['content'].str.lower() #reduces all letters in the comments to lower case\n",
        "df['content'].replace( { r'[^a-zA-Z ]' : '' }, inplace= True, regex = True) #anything other than (a-z)(A-Z)(space) will be removed\n",
        "                                                                                 # ^ means \"not\" in regex\n",
        "df['title'] = df['title'].str.lower()\n",
        "df['title'].replace( { r'[^a-zA-Z ]' : '' }, inplace= True, regex = True)\n",
        "\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "        text: a string\n",
        "        \n",
        "        return: modified initial string\n",
        "    \"\"\"\n",
        "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
        "    text = text.lower() # lowercase text\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
        "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
        "    return text\n",
        "    \n",
        "df['content'] = df['content'].apply(clean_text)\n",
        "df['title'] = df['title'].apply(clean_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMntCEEEYv9Q"
      },
      "source": [
        "df['X']=df['title']+df['content']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKW4AcEbYpMc"
      },
      "source": [
        "# The maximum number of words to be used. (most frequent)\n",
        "MAX_NB_WORDS = 70000\n",
        "# Max number of words in each complaint.\n",
        "MAX_SEQUENCE_LENGTH = 250\n",
        "# This is fixed.\n",
        "EMBEDDING_DIM = 100\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
        "tokenizer.fit_on_texts(df['X'].values)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sWALXobZOhA"
      },
      "source": [
        "X = tokenizer.texts_to_sequences(df['X'].values)\n",
        "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensor:', X.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lE6vwzGZYay"
      },
      "source": [
        "Y = pd.get_dummies(df['target_ind']).values\n",
        "print('Shape of label tensor:', Y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6fqNA81ZYmx"
      },
      "source": [
        "X_train=X\n",
        "Y_train=Y\n",
        "print(X_train.shape,Y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUWuVP-FZYv3"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(500, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiSHWLDFiEjk"
      },
      "source": [
        "model=torch.load('/content/gdrive/MyDrive/Pre-Inter IIT/LSTMmodel.pth') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ajy1kiBt8n1X"
      },
      "source": [
        "#val=38.83\n",
        "epochs = 1\n",
        "batch_size = 1024\n",
        "\n",
        "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.15,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DGQtK7rh6lV"
      },
      "source": [
        "torch.save(model, '/content/gdrive/MyDrive/Pre-Inter IIT/LSTMmodel.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdX-eSImHm5F"
      },
      "source": [
        "test_df=pd.read_csv('/content/test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsB7d7WRkpYA"
      },
      "source": [
        "test_df['content'] = test_df['content'].str.lower() #reduces all letters in the comments to lower case\n",
        "test_df['content'].replace( { r'[^a-zA-Z ]' : '' }, inplace= True, regex = True) #anything other than (a-z)(A-Z)(space) will be removed\n",
        "                                                                                 # ^ means \"not\" in regex\n",
        "test_df['title'] = test_df['title'].str.lower()\n",
        "test_df['title'].replace( { r'[^a-zA-Z ]' : '' }, inplace= True, regex = True)\n",
        "\n",
        "test_df['content'] = test_df['content'].apply(clean_text)\n",
        "test_df['title'] = test_df['title'].apply(clean_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-3LB1kzkwxe"
      },
      "source": [
        "test_df['X']=test_df['title']+test_df['content']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1kNk95wk0eC"
      },
      "source": [
        "X = tokenizer.texts_to_sequences(test_df['X'].values)\n",
        "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf_LPvczk_TC"
      },
      "source": [
        "preds=model.predict(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-4OoJI0lHOn"
      },
      "source": [
        "preds=np.argmax(preds,axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMIg8swqlOwV"
      },
      "source": [
        "preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_0W4QLRlh4N"
      },
      "source": [
        "test_df['target_ind']=preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPd02JyNlan9"
      },
      "source": [
        "test_df=test_df.drop(['content', 'title', 'X'],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYwVV1KulZ4z"
      },
      "source": [
        "test_df.to_csv('/content/submission.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfCxqeZlsH0t"
      },
      "source": [
        "#Bidirectional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBKyjD0sjA4R"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import model_selection\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_RN-UJ2jW4-"
      },
      "source": [
        "df['content'] = df['content'].str.lower() #reduces all letters in the comments to lower case\n",
        "df['content'].replace( { r'[^a-zA-Z ]' : '' }, inplace= True, regex = True) #anything other than (a-z)(A-Z)(space) will be removed\n",
        "                                                                                 # ^ means \"not\" in regex\n",
        "df['title'] = df['title'].str.lower()\n",
        "df['title'].replace( { r'[^a-zA-Z ]' : '' }, inplace= True, regex = True)\n",
        "\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "        text: a string\n",
        "        \n",
        "        return: modified initial string\n",
        "    \"\"\"\n",
        "    text = text.lower() # lowercase text\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
        "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
        "    return text\n",
        "    \n",
        "df['content'] = df['content'].apply(clean_text)\n",
        "df['title'] = df['title'].apply(clean_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqtM0FBRjFHM"
      },
      "source": [
        "train_df=df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5ldPhg-jI9I"
      },
      "source": [
        "max_features=10000 #we set maximum number of words to 10000\n",
        "maxlen=1 #we set maximum sequence length to 10000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c84nzseLjls2"
      },
      "source": [
        "tok = tf.keras.preprocessing.text.Tokenizer(num_words=max_features) #again tokenizer step\n",
        "tok.fit_on_texts(list(train_df['content'])+list(train_df['title'])) #fit to cleaned text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxXorL29k0J6"
      },
      "source": [
        "print(len(tok.word_index))\n",
        "vocab_size = len(tok.word_index) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAO9bMv0ksDM"
      },
      "source": [
        "text_df = tok.texts_to_sequences(list(train_df['content'])) #this is how we create sequences\n",
        "text_df = tf.keras.preprocessing.sequence.pad_sequences(text_df, maxlen=maxlen) #let's execute pad step\n",
        "\n",
        "title_df = tok.texts_to_sequences(list(train_df['title'])) #this is how we create sequences\n",
        "title_df = tf.keras.preprocessing.sequence.pad_sequences(title_df, maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBl73R7zlB_R"
      },
      "source": [
        "train_df = title_df + text_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUwAx64ulZpp"
      },
      "source": [
        "#One-Hot Encoding for the Target Categories\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "categorical_cols=['target_ind']\n",
        "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore').fit(df[categorical_cols])\n",
        "encoded_cols = list(encoder.get_feature_names(categorical_cols))\n",
        "target_output = encoder.transform(df[categorical_cols])\n",
        "\n",
        "Y=target_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKSXkSaulPGH"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_df, Y, test_size=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9-rwEOTBaA7"
      },
      "source": [
        "X_train=X_train[:2000]\n",
        "y_train=y_train[:2000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EqPIXlBlSHa"
      },
      "source": [
        "embedding_dim = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1QvK83dllM_"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim, return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    tf.keras.layers.Dense(embedding_dim, activation='relu'),\n",
        "    tf.keras.layers.Dense(500, activation=tf.nn.softmax)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FYtcG6YlpNg"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',#no more categorical_crossentropy\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Da9fHggkltfi"
      },
      "source": [
        "model.fit(np.array(X_train), np.array(y_train), epochs=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15TE2qA-mGn-"
      },
      "source": [
        "test_df['content'] = test_df['content'].str.lower() #reduces all letters in the comments to lower case\n",
        "test_df['content'].replace( { r'[^a-zA-Z ]' : '' }, inplace= True, regex = True) #anything other than (a-z)(A-Z)(space) will be removed\n",
        "                                                                                 # ^ means \"not\" in regex\n",
        "test_df['title'] = test_df['title'].str.lower()\n",
        "test_df['title'].replace( { r'[^a-zA-Z ]' : '' }, inplace= True, regex = True)\n",
        "\n",
        "test_df['content'] = test_df['content'].apply(clean_text)\n",
        "test_df['title'] = test_df['title'].apply(clean_text)\n",
        "\n",
        "text_df = tok.texts_to_sequences(list(test_df['content'])) #this is how we create sequences\n",
        "text_df = tf.keras.preprocessing.sequence.pad_sequences(text_df, maxlen=maxlen) #let's execute pad step\n",
        "\n",
        "title_df = tok.texts_to_sequences(list(test_df['title'])) #this is how we create sequences\n",
        "title_df = tf.keras.preprocessing.sequence.pad_sequences(title_df, maxlen=maxlen)\n",
        "\n",
        "test_df = title_df + text_df\n",
        "X_test=test_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfo07lpJ-BNp"
      },
      "source": [
        "test_df=pd.read_csv('/content/test.csv')\n",
        "test_df.drop('title',axis=1,inplace=True)\n",
        "test_df.drop('content',axis=1,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcVyTao4-hIc"
      },
      "source": [
        "test_df['target_ind']=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBD-iHJ1DLX4"
      },
      "source": [
        "model.predict(X_test[0]).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-Q4JGQE-uX7"
      },
      "source": [
        "np.argmax(model.predict(X_test[9]),axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27MnjV3OEMDK"
      },
      "source": [
        "np.argmax(model.predict(X_test[12]),axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hG8G9gXl5SI"
      },
      "source": [
        "for i in range(0,len(X_test)):\n",
        "   preds=model.predict(X_test[i])\n",
        "   test_df.loc[i,'target_ind']=preds\n",
        "   if i%100==0:\n",
        "     print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYbDuc59l4Fi"
      },
      "source": [
        "test_df.to_csv('submission.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oH4aBCDmBWl"
      },
      "source": [
        "#LinearSVC \n",
        "This model gave the highest accuracy with least inferance time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riOgKzVRhOku"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "df['content'] = df['content'].str.lower() #reduces all letters in the comments to lower case\n",
        "df['content'].replace( { r'[^a-zA-Z ]' : '' }, inplace= True, regex = True) #anything other than (a-z)(A-Z)(space) will be removed\n",
        "                                                                                 # ^ means \"not\" in regex\n",
        "df['title'] = df['title'].str.lower()\n",
        "df['title'].replace( { r'[^a-zA-Z ]' : '' }, inplace= True, regex = True)\n",
        "\n",
        "df['title'].replace( { 'amazoncom' : '' }, inplace= True, regex = True)\n",
        "df['content'].replace( { 'amazoncom' : '' }, inplace= True, regex = True)\n",
        "\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "        text: a string\n",
        "        \n",
        "        return: modified initial string\n",
        "    \"\"\"\n",
        "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
        "    text = text.lower() # lowercase text\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
        "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
        "    return text\n",
        "    \n",
        "df['content'] = df['content'].apply(clean_text)\n",
        "df['title'] = df['title'].apply(clean_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmY5HQu9iO9w"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBm9P49OuoCZ"
      },
      "source": [
        "df['X']=df['title']+' '+df['title']+' '+df['title']+' '+df['content']\n",
        "#df['X']=df['content']\n",
        "#df['X']=df['title']\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "def stem_words(text):\n",
        "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
        "\n",
        "df[\"X\"] = df[\"X\"].apply(lambda text: stem_words(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzujVwuTbpNS"
      },
      "source": [
        "df.drop('title',axis=1,inplace=True)\n",
        "df.drop('content',axis=1,inplace=True)\n",
        "\n",
        "X_train=df['X']\n",
        "y_train=df['target_ind']\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopwords = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOgI0ydyhHAU"
      },
      "source": [
        "# Let's first try with Count Vectorizer from scikit learn\n",
        "\n",
        "cv = TfidfVectorizer(stop_words=stopwords)\n",
        "X_train_cv = cv.fit_transform(X_train)\n",
        "X_train_cv.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBcdO9kmPOWz"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "clf = LinearSVC()\n",
        "clf.fit(X_train_cv,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Skdd5owMMkKD"
      },
      "source": [
        "test_df=pd.read_csv('/content/test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4bywxm6iGKy"
      },
      "source": [
        "test_df['content'] = test_df['content'].str.lower() #reduces all letters in the comments to lower case\n",
        "test_df['content'].replace( { r'[^a-zA-Z ]' : '' }, inplace= True, regex = True) #anything other than (a-z)(A-Z)(space) will be removed\n",
        "                                                                                 # ^ means \"not\" in regex\n",
        "test_df['title'] = test_df['title'].str.lower()\n",
        "test_df['title'].replace( { r'[^a-zA-Z ]' : '' }, inplace= True, regex = True)\n",
        "\n",
        "test_df['content'] = test_df['content'].apply(clean_text)\n",
        "test_df['title'] = test_df['title'].apply(clean_text)\n",
        "\n",
        "test_df['X']=test_df['title']+' '+test_df['title']+' '+test_df['title']+' '+test_df['content']\n",
        "#X_test=test_df['content']\n",
        "#X_test=test_df['title']\n",
        "\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "def stem_words(text):\n",
        "   return \" \".join([stemmer.stem(word) for word in text.split()])\n",
        "\n",
        "test_df[\"X\"] = test_df[\"X\"].apply(lambda text: stem_words(text))\n",
        "\n",
        "X_test=test_df['X']\n",
        "\n",
        "X_test_cv = cv.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOoEXuK0h4Yv"
      },
      "source": [
        "preds=clf.predict(X_test_cv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UIAlwvik4kd"
      },
      "source": [
        "test_df['target_ind']=preds\n",
        "test_df.drop('title',axis=1,inplace=True)\n",
        "test_df.drop('content',axis=1,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQl4ESSBdjyh"
      },
      "source": [
        "test_df.drop('X',axis=1,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcy3cxpLNSt8"
      },
      "source": [
        "test_df.to_csv('submission.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzrCKunxw5s6"
      },
      "source": [
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Coeu7BdyvEZG"
      },
      "source": [
        "filename = 'finalized_model.sav'\n",
        "pickle.dump(clf, open(filename, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXvfIGehvE_w"
      },
      "source": [
        "# load the model from disk\n",
        "loaded_model = pickle.load(open(filename, 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UG0SZ1K1BtC"
      },
      "source": [
        "filename = 'TfidfVectorizer.sav'\n",
        "pickle.dump(cv, open(filename, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiFi3dArwzi7"
      },
      "source": [
        "#MLP Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdlfsxxKeCXa"
      },
      "source": [
        "df['content'] = df['content'].str.lower() #reduces all letters in the comments to lower case\n",
        "df['content'].replace( { r'[^a-zA-Z0-9, ]' : '' }, inplace= True, regex = True) #anything other than (a-z)(A-Z)(0-9)(comma)(space) will be removed\n",
        "                                                                                 # ^ means \"not\" in regex\n",
        "\n",
        "#df['X']=df['title']+df['content']\n",
        "df['X']=df['content']\n",
        "\n",
        "df.drop('title',axis=1,inplace=True)\n",
        "df.drop('content',axis=1,inplace=True)\n",
        "\n",
        "X_train=df['X']\n",
        "y_train=df['target_ind']\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopwords = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFwkfQ3ayZ2t"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "cv = TfidfVectorizer(stop_words=stopwords)\n",
        "X_train_cv = cv.fit_transform(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FciKKBQoyZyB"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "mlp=MLPClassifier()\n",
        "mlp.fit(X_train_cv[:],y_train[:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1tb0G2JyZ7B"
      },
      "source": [
        "#X_test=test_df['title']+test_df['content']\n",
        "X_test=test_df['content']\n",
        "X_test_cv = cv.transform(X_test)\n",
        "mlp_prediction=mlp.predict(X_test_cv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-wHQlw2yaBR"
      },
      "source": [
        "test_df['target_ind']=mlp_prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQNf0Pyx0rf7"
      },
      "source": [
        "test_df.drop('title',axis=1,inplace=True)\n",
        "test_df.drop('content',axis=1,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFroweSL0wVC"
      },
      "source": [
        "test_df.to_csv('submission.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cicVAMJ9qU9s"
      },
      "source": [
        "#Using Glove Embeddings: \n",
        "*make separate lstms for predicting using content and title and then ensemble them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YYUOzsLeA0v"
      },
      "source": [
        "import spacy\n",
        "spacy_eng = spacy.load(\"en\")\n",
        "\n",
        "class Vocabulary:\n",
        "    PAD_token = 0   # Used for padding short sentences\n",
        "    SOS_token = 1   # Start-of-sentence token\n",
        "    EOS_token = 2   # End-of-sentence token\n",
        "\n",
        "    def __init__(self, name): #creating various dictionaries\n",
        "        self.name = name\n",
        "        self.word2index = {'PAD': 0, 'SOS': 1, 'EOS': 2} #maps words to indexes\n",
        "        self.word2count = {} #counts the number of times a particular word occurs\n",
        "        self.index2word = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\"} #maps indexes to words\n",
        "        self.num_words = 3 #total number of unique words/tokens (including EOS, SOS and PAD)\n",
        "        self.num_sentences = 0 \n",
        "        self.longest_sentence = 0\n",
        "    \n",
        "\n",
        "\n",
        "    def add_word(self, word): #adding words/tokens to the several dictionaries\n",
        "        if word not in self.word2index:\n",
        "            # First entry of word into vocabulary (if the word doesnt already exist in word2index and index2word; add it to the same)\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            # If Word already exists in word2index and index2word; increase word count\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    def tokenizer(self,text): #Tokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens. The tokens could be words, numbers or punctuation marks.\n",
        "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
        "    \n",
        "    def add_sentence(self, sentence):\n",
        "        sentence_len = 0\n",
        "        #for word in sentence.split(' '):\n",
        "        for word in self.tokenizer(sentence):\n",
        "            sentence_len += 1\n",
        "            self.add_word(word)\n",
        "        if sentence_len > self.longest_sentence:\n",
        "            # This is the longest sentence\n",
        "            self.longest_sentence = sentence_len\n",
        "        # Count the number of sentences\n",
        "        self.num_sentences += 1\n",
        "\n",
        "    def to_word(self, index):\n",
        "        return self.index2word[index]\n",
        "\n",
        "    def to_index(self, word):\n",
        "        return self.word2index[word]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jHC03QW7Id5"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "df['content'] = df['content'].str.lower() #reduces all letters in the comments to lower case\n",
        "df['content'].replace( { r'[^a-zA-Z ]' : '' }, inplace= True, regex = True) #anything other than (a-z)(A-Z)(space) will be removed\n",
        "                                                                                 # ^ means \"not\" in regex\n",
        "df['title'] = df['title'].str.lower()\n",
        "df['title'].replace( { r'[^a-zA-Z ]' : '' }, inplace= True, regex = True)\n",
        "\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "        text: a string\n",
        "        \n",
        "        return: modified initial string\n",
        "    \"\"\"\n",
        "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
        "    text = text.lower() # lowercase text\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
        "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
        "    return text\n",
        "    \n",
        "df['content'] = df['content'].apply(clean_text)\n",
        "df['title'] = df['title'].apply(clean_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx9yEXbVE1Yh"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GU9SafzE4nY"
      },
      "source": [
        "# creating the vocabulary \n",
        "vocab = Vocabulary('test')\n",
        "\n",
        "# adding words to the vocabulary \n",
        "for sentence in df['content']:\n",
        "    vocab.add_sentence(sentence) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6KY5YB0St19"
      },
      "source": [
        "l=[]\n",
        "for i in vocab.word2count.keys():\n",
        "  if vocab.word2count[i]>1000:\n",
        "    l.append(i)\n",
        "l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH-uta1eeGRc"
      },
      "source": [
        "# converting the captions to tokens \n",
        "sent_idxs = [1] # sent_idxs is a list that will contain the indexes representing each word/token of the sentance\n",
        "captions = [] # captions is a list that will contain the sent_idxs lists of all sentances\n",
        "for idx, sentence in enumerate (df['content']): #Enumerate returns the index(idx) and the value inside that index(sentance)\n",
        "    i = 0                                        # we use idx to just keep track of the iterations\n",
        "    '''for word in sentence.split(' '):\n",
        "        if i==0:\n",
        "            sent_idxs.append(1)\n",
        "        index = vocab.to_index(word)    \n",
        "        if index != 3:    \n",
        "            sent_idxs.append(index)\n",
        "        elif index == 3 and i !=0:\n",
        "            sent_idxs.append(2)\n",
        "        i+=1'''\n",
        "    #for word in sentence.split(' '):\n",
        "    for word in vocab.tokenizer(sentence):\n",
        "        sent_idxs.append(vocab.to_index(word))\n",
        "        i+=1\n",
        "    while i < (9396): #padding the sentances until length of the sentace becomes equal to the length of the longest sentance\n",
        "        sent_idxs.append(0)\n",
        "        i+=1\n",
        "    captions.append(sent_idxs)\n",
        "    sent_idxs = [1]\n",
        "\n",
        "# converting list of word tokens to numpy array\n",
        "captions = np.array(captions)\n",
        "captions.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qKQMZ6qs3WE"
      },
      "source": [
        "!kaggle datasets download -d anindya2906/glove6b \n",
        "!unzip \"/content/glove6b.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbY0WaS8tec1"
      },
      "source": [
        "import numpy as np \n",
        "vocabs = vocab.word2index.keys()\n",
        "\n",
        "def load_embeds(root_dir):\n",
        "    embeddings_index = dict()\n",
        "    f = open(root_dir)\n",
        "\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "    f.close()\n",
        "    return embeddings_index\n",
        "    \n",
        "embeddings_index = load_embeds('../content/glove.6B.300d.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZOn1jgrtESp"
      },
      "source": [
        "def load_embed_weights(embeddings_index, embed_dim, vocab, vocab_size):\n",
        "    matrix_len = vocab_size\n",
        "    weights_matrix = np.zeros((matrix_len, embed_dim))\n",
        "    words_found = 0\n",
        "\n",
        "    for i, word in enumerate(vocab):\n",
        "        try: \n",
        "            weights_matrix[i] = embeddings_index[word]\n",
        "            words_found += 1\n",
        "        #but if the embedding for that word is not found in the glove embeddings, then:    \n",
        "        except KeyError: \n",
        "            weights_matrix[i] = np.random.normal(scale=0.6, size=(embed_dim, ))\n",
        "\n",
        "    weights_matrix = torch.tensor(weights_matrix)\n",
        "    return weights_matrix\n",
        "\n",
        "\n",
        "weights_matrix = load_embed_weights(embeddings_index, 300, vocabs, vocab.num_words)\n",
        "weights_matrix.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HCoI-_rygoB"
      },
      "source": [
        "captions.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feRmygwlfOd3"
      },
      "source": [
        "# adding end tokens\n",
        "for i in range(len(captions)):\n",
        "    for j in range(1,len(captions[1])):\n",
        "        if captions[i,j] == 0:\n",
        "                captions[i,j] = 2 #we replace the first PAD with EOS\n",
        "                break  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veYUOsjr-CPO"
      },
      "source": [
        "#One-Hot Encoding for the Target Categories\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "categorical_cols=['target_ind']\n",
        "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore').fit(df[categorical_cols])\n",
        "encoded_cols = list(encoder.get_feature_names(categorical_cols))\n",
        "target_output = encoder.transform(df[categorical_cols])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vClpREXzKSK"
      },
      "source": [
        "embed = nn.Embedding(num_embeddings = len(vocabs), embedding_dim = 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3hUvpg-fv1e"
      },
      "source": [
        "x_train = np.array(captions[0:100])\n",
        "y_train =  np.array(target_output[0:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyPnuequWSrm"
      },
      "source": [
        "del(captions)\n",
        "del(target_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UM5Ol3Z8XDo"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIzofXzLV7lk"
      },
      "source": [
        "x_train = np.reshape(x_train, (x_train.shape[0] , 1, 9397, 1))\n",
        "y_train = np.reshape(y_train, (y_train.shape[0] , 500, 1))\n",
        "#x_train = torch.from_numpy(x_train)\n",
        "#y_train = torch.from_numpy(y_train)\n",
        "x_train = torch.tensor(x_train)\n",
        "y_train = torch.tensor(y_train)\n",
        "x_train=embed(x_train)\n",
        "x_train=x_train.detach().numpy()\n",
        "x_train = np.reshape(x_train, (x_train.shape[0] , 9397, 300, 1))\n",
        "x_train = torch.tensor(x_train)\n",
        "x_train = x_train.type(torch.FloatTensor)\n",
        "y_train = y_train.type(torch.FloatTensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPRIooZtKqIF"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fFDXS7IWXWm"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "trainset = [(x_train[i], y_train[i]) for i in range(len(x_train))]\n",
        "batch_size = 32\n",
        "trainloader = DataLoader(dataset=trainset, batch_size=batch_size, shuffle = True)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chNBmpwTWa6Y"
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "  \n",
        "  def __init__(self):\n",
        "    \n",
        "    super().__init__()\n",
        "    self.lstm = nn.LSTM(300,512, batch_first = True)\n",
        "    self.fc = nn.Linear(512,500)\n",
        "    self.relu = nn.ReLU()\n",
        "    \n",
        "  def forward(self,x):\n",
        "    out, _ = self.lstm(x)(embedding_layer)\n",
        "    out = self.fc(self.relu(out[:,-1,:]))\n",
        "    return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxRgeTKeWh42"
      },
      "source": [
        "import numpy as np\n",
        "best_loss = 0 \n",
        "\n",
        "model = LSTM(300,vocab_size,weights_matrix)\n",
        "\n",
        "model.to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "epochs = 2\n",
        "for epoch in range(epochs):\n",
        "  trainloss = []\n",
        "  for idx, (x,y) in enumerate(trainloader): \n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    preds = model(x.squeeze(-1))\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(preds, y.squeeze(-1))\n",
        "    trainloss.append(loss.item())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if idx%250 == 0:\n",
        "      print(f'epoch:{epoch+1}({idx*100/len(trainloader)}%)\\t loss: {np.mean(trainloss)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vuRC5eZd1GL"
      },
      "source": [
        "model=torch.load('/content/gdrive/MyDrive/Pre-Inter IIT/model.pth',map_location=torch.device('cpu'))\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff7WdOK6ZtDY"
      },
      "source": [
        "del(x_train)\n",
        "del(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8_x1oJ1_HFl"
      },
      "source": [
        "torch.save(model, '/content/gdrive/MyDrive/Pre-Inter IIT/model.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-voI9UMV76Fh"
      },
      "source": [
        "test_df['content'] = test_df['content'].str.lower() #reduces all letters in the comments to lower case\n",
        "test_df['content'].replace( { r'[^a-zA-Z ]' : '' }, inplace= True, regex = True) #anything other than (a-z)(A-Z)(space) will be removed\n",
        "                                                                                 # ^ means \"not\" in regex\n",
        "test_df['title'] = test_df['title'].str.lower()\n",
        "test_df['title'].replace( { r'[^a-zA-Z ]' : '' }, inplace= True, regex = True)\n",
        "\n",
        "test_df['content'] = test_df['content'].apply(clean_text)\n",
        "test_df['title'] = test_df['title'].apply(clean_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETI3pY_S3etk"
      },
      "source": [
        "# adding words to the vocabulary \n",
        "for sentence in test_df['content']:\n",
        "    vocab.add_sentence(sentence) \n",
        "\n",
        "# converting the captions to tokens \n",
        "sent_idxs = [1] # sent_idxs is a list that will contain the indexes representing each word/token of the sentance\n",
        "captions = [] # captions is a list that will contain the sent_idxs lists of all sentances\n",
        "for idx, sentence in enumerate (test_df['content']): #Enumerate returns the index(idx) and the value inside that index(sentance)\n",
        "    i = 0                                        # we use idx to just keep track of the iterations\n",
        "    '''for word in sentence.split(' '):\n",
        "        if i==0:\n",
        "            sent_idxs.append(1)\n",
        "        index = vocab.to_index(word)    \n",
        "        if index != 3:    \n",
        "            sent_idxs.append(index)\n",
        "        elif index == 3 and i !=0:\n",
        "            sent_idxs.append(2)\n",
        "        i+=1'''\n",
        "    #for word in sentence.split(' '):\n",
        "    for word in vocab.tokenizer(sentence):\n",
        "        sent_idxs.append(vocab.to_index(word))\n",
        "        i+=1\n",
        "    while i < (9396): #padding the sentances until length of the sentace becomes equal to the length of the sentance the lstm model is trained on\n",
        "        sent_idxs.append(0)\n",
        "        i+=1\n",
        "    captions.append(sent_idxs)\n",
        "    sent_idxs = [1]\n",
        "\n",
        "# converting list of word tokens to numpy array\n",
        "captions = np.array(captions)\n",
        "\n",
        "# adding end tokens\n",
        "for i in range(len(captions)):\n",
        "    for j in range(1,len(captions[1])):\n",
        "        if captions[i,j] == 0:\n",
        "                captions[i,j] = 2 #we replace the first PAD with EOS\n",
        "                break  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYgokTmSKCV_"
      },
      "source": [
        "del(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRWRRkAcEqeG"
      },
      "source": [
        "test_df.drop('title',axis=1, inplace=True)\n",
        "test_df.drop('content',axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzMuB4wGFLKM"
      },
      "source": [
        "test_df['target_ind']=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtNXCsZOeWh1"
      },
      "source": [
        "embed = nn.Embedding(num_embeddings = len(vocabs), embedding_dim = 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBAO_lOm7DgL"
      },
      "source": [
        "x=embed(captions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5L50ooK3H7C"
      },
      "source": [
        "captions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9HyBovmFVpe"
      },
      "source": [
        "x_test = np.array(captions[8])\n",
        "x_test = np.reshape(x_test, (1 , 1, 9397, 1))\n",
        "x_test = torch.tensor(x_test)\n",
        "X_test=embed(x_test)\n",
        "del(x_test)\n",
        "x_test=X_test.detach().numpy()\n",
        "del(X_test)\n",
        "x_test = np.reshape(x_test, (1 , 9397, 300, 1))\n",
        "x_test = torch.tensor(x_test)\n",
        "x_test = x_test.type(torch.FloatTensor)\n",
        "model.to('cpu')\n",
        "torch.cuda.empty_cache()\n",
        "pred = model(x_test.squeeze(-1))\n",
        "pred=pred.cpu()\n",
        "pred=pred.detach().numpy()\n",
        "target_ind=np.argmax(pred,axis=1)\n",
        "target_ind"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrgEWOu818HI"
      },
      "source": [
        "x_test = np.array(captions[0])\n",
        "x_test = np.reshape(x_test, (1 , 1, 9397, 1))\n",
        "x_test = torch.tensor(x_test)\n",
        "X_test=embed(x_test)\n",
        "del(x_test)\n",
        "x_test=X_test.detach().numpy()\n",
        "del(X_test)\n",
        "x_test = np.reshape(x_test, (1 , 9397, 300, 1))\n",
        "x_test = torch.tensor(x_test)\n",
        "x_test = x_test.type(torch.FloatTensor)\n",
        "model.to('cpu')\n",
        "torch.cuda.empty_cache()\n",
        "pred = model(x_test.squeeze(-1))\n",
        "pred=pred.cpu()\n",
        "pred=pred.detach().numpy()\n",
        "target_ind=np.argmax(pred,axis=1)\n",
        "target_ind"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lA-PyhmJJuu"
      },
      "source": [
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzIr12O1McMm"
      },
      "source": [
        "for i in test_df.index:\n",
        " x1 = np.array(captions[i])\n",
        " x2= np.reshape(x1, (1, 1, 9397, 1))\n",
        " del(x1)\n",
        " x3 = torch.tensor(x2)\n",
        " del(x2)\n",
        " x4=embed(x3)\n",
        " del(x3)\n",
        " x5=x4.detach().numpy()\n",
        " del(x4)\n",
        " x6 = np.reshape(x5, (1 , 9397, 300, 1))\n",
        " del(x5)\n",
        " x7 = torch.tensor(x6)\n",
        " del(x6)\n",
        " x8 = x7.type(torch.FloatTensor)\n",
        " del(x7)\n",
        " x9=x8.to(device)\n",
        " del(x8)\n",
        " pred = model(x9.squeeze(-1))\n",
        " del(x9)\n",
        " pred=pred.cpu()\n",
        " pred=pred.detach().numpy()\n",
        " target_ind=np.argmax(pred,axis=1)\n",
        " del(pred)\n",
        " test_df.loc[i,'target_ind']=target_ind\n",
        " print(i,\" \",target_ind)\n",
        " del(target_ind)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuDhNKNpKOSN"
      },
      "source": [
        "test_df.to_csv('/content/submission.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWo4fTudU7OR"
      },
      "source": [
        "test_df"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}